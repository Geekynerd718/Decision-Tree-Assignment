{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q.1 What is a Decision Tree, and how does it work?\n",
        "\n",
        "Ans - A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. It's kind of like playing “20 Questions” with your data—systematically narrowing down possibilities based on answers to yes/no or threshold-based questions.\n",
        "How it works:\n",
        "- Starts at the Root Node: The tree begins with a single node (the “root”), which asks a question based on a feature in your dataset. For example: “Is the temperature > 30°C?”\n",
        "- Branches Out: Based on the answer (yes or no, or > threshold vs. ≤ threshold), the data gets split. Each branch leads to another node that asks another question, and the process continues.\n",
        "- Reaches Leaf Nodes: Eventually, you hit a node where no further splitting is needed. This is a leaf node, where the model makes a prediction (like a class label or numeric value).\n",
        "- Makes Predictions: To predict, you start at the root and move down the tree following the answers to the questions until you land at a leaf node.\n"
      ],
      "metadata": {
        "id": "sgQYDIAu9iY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.2 What are impurity measures in Decision Trees?\n",
        "\n",
        "AnsImpurity measures in decision trees quantify how mixed or uncertain the class labels are at a node. They help determine the best feature to split the data, aiming to create purer (more homogeneous) child nodes. Lower impurity indicates a node with mostly one class, while higher impurity means mixed classes. Common impurity measures include:\n",
        "\n",
        "1. **Gini Impurity**:\n",
        "   - Formula: \\( Gini = 1 - \\sum_{i=1}^n p_i^2 \\), where \\( p_i \\) is the proportion of class \\( i \\) in the node.\n",
        "   - Range: 0 (pure, all one class) to 0.5 (maximum impurity for two classes).\n",
        "   - Measures the probability of misclassifying a randomly chosen element if labeled randomly based on the node's class distribution.\n",
        "   - Computationally efficient and widely used.\n",
        "\n",
        "2. **Entropy (Information Gain)**:\n",
        "   - Formula: \\( Entropy = - \\sum_{i=1}^n p_i \\log_2(p_i) \\), where \\( p_i \\) is the proportion of class \\( i \\).\n",
        "   - Range: 0 (pure) to 1 (maximum impurity for two classes).\n",
        "   - Measures the level of disorder or uncertainty in the node.\n",
        "   - Used in algorithms like ID3 and C4.5. Information Gain is the reduction in entropy after a split.\n",
        "\n",
        "3. **Misclassification Error**:\n",
        "   - Formula: \\( Error = 1 - \\max(p_i) \\), where \\( \\max(p_i) \\) is the proportion of the most common class.\n",
        "   - Range: 0 (pure) to 0.5 (maximum impurity).\n",
        "   - Measures the fraction of misclassified instances if all are assigned the majority class.\n",
        "   - Less sensitive to class distribution changes, so less commonly used.\n",
        "\n"
      ],
      "metadata": {
        "id": "9Bbc_DQltygh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.3 What is the mathematical formula for Gini Impurity?\n",
        "\n",
        "Ans - The mathematical formula for Gini Impurity is:\n",
        "\n",
        "\\[ Gini = 1 - \\sum_{i=1}^n p_i^2 \\]\n",
        "\n",
        "where:\n",
        "- \\( p_i \\) is the proportion (or probability) of class \\( i \\) in the node.\n",
        "- \\( n \\) is the number of classes.\n",
        "\n",
        "This formula calculates the probability of misclassifying a randomly chosen element in the node if it were labeled randomly according to the class distribution. The value ranges from 0 (pure node, all one class) to 0.5 (maximum impurity for two equally distributed classes).\n"
      ],
      "metadata": {
        "id": "eADwvKThuNQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.4 What is the mathematical formula for Entropy?\n",
        "\n",
        "Ans - The mathematical formula for Entropy is:\n",
        "\n",
        "\\[ Entropy = - \\sum_{i=1}^n p_i \\log_2(p_i) \\]\n",
        "\n",
        "where:\n",
        "- \\( p_i \\) is the proportion (or probability) of class \\( i \\) in the node.\n",
        "- \\( n \\) is the number of classes.\n",
        "- \\( \\log_2 \\) is the base-2 logarithm.\n",
        "\n",
        "This measures the level of uncertainty or disorder in a node, with values ranging from 0 (pure, all one class) to 1 (maximum impurity for two equally distributed classes). If \\( p_i = 0 \\), the term \\( p_i \\log_2(p_i) \\) is defined as 0 to handle the limit.\n"
      ],
      "metadata": {
        "id": "lRRCwxpGuV_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.5 What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Ans - **Information Gain** is a metric used in decision trees to measure the reduction in entropy (or impurity) achieved by splitting a node based on a specific feature. It quantifies how much information a feature provides in separating the classes, helping to decide the best feature to split on at each node.\n",
        "\n",
        "### Mathematical Formula for Information Gain\n",
        "\\[ \\text{Information Gain} = \\text{Entropy(parent)} - \\sum_{j=1}^m \\frac{N_j}{N} \\cdot \\text{Entropy(child}_j) \\]\n",
        "where:\n",
        "- \\( \\text{Entropy(parent)} \\): Entropy of the parent node before the split, calculated as \\( -\\sum_{i=1}^n p_i \\log_2(p_i) \\), where \\( p_i \\) is the proportion of class \\( i \\) in the parent node.\n",
        "- \\( m \\): Number of child nodes created by the split.\n",
        "- \\( N_j \\): Number of instances in child node \\( j \\).\n",
        "- \\( N \\): Total number of instances in the parent node.\n",
        "- \\( \\text{Entropy(child}_j) \\): Entropy of child node \\( j \\).\n",
        "- The term \\( \\frac{N_j}{N} \\) weights the entropy of each child node by its proportion of instances.\n",
        "\n",
        "### How Information Gain is Used in Decision Trees\n",
        "1. **Feature Selection**:\n",
        "   - At each node, the decision tree algorithm calculates the Information Gain for each possible feature (and split point for continuous features).\n",
        "   - The feature with the highest Information Gain is chosen for the split, as it reduces entropy (uncertainty) the most, creating purer child nodes.\n",
        "\n",
        "2. **Building the Tree**:\n",
        "   - The algorithm recursively splits nodes by selecting the feature that maximizes Information Gain until a stopping criterion is met (e.g., maximum depth, minimum samples per node, or pure nodes).\n",
        "   - This process ensures that each split improves class separation, leading to a tree that better predicts the target variable.\n",
        "\n",
        "3. **Algorithms**:\n",
        "   - Information Gain is primarily used in algorithms like ID3 and C4.5, which rely on entropy as the impurity measure. (Note: Some algorithms like CART use Gini Impurity instead, but Information Gain can be adapted for Gini as well.)\n",
        "\n",
        "### Example\n",
        "Suppose a parent node has 100 instances (50 class A, 50 class B, entropy = 1). Splitting on feature \\( X \\) creates two child nodes:\n",
        "- Child 1: 60 instances (50 class A, 10 class B, entropy = 0.65).\n",
        "- Child 2: 40 instances (0 class A, 40 class B, entropy = 0).\n",
        "The weighted entropy after the split is:\n",
        "\\[ \\text{Weighted Entropy} = \\frac{60}{100} \\cdot 0.65 + \\frac{40}{100} \\cdot 0 = 0.39 \\]\n",
        "Information Gain is:\n",
        "\\[ \\text{Information Gain} = 1 - 0.39 = 0.61 \\]\n",
        "This gain is compared with other features’ gains to choose the best split.\n",
        "\n"
      ],
      "metadata": {
        "id": "nFX280vWufrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.6 What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Ans - Gini Impurity and Entropy are both impurity measures used in decision trees to evaluate how mixed the class labels are at a node and guide the selection of the best feature for splitting. While they serve the same purpose, they differ in their mathematical formulation, interpretation, and computational properties. Below is a concise comparison:\n",
        "\n",
        "### 1. **Mathematical Formula**\n",
        "- **Gini Impurity**:\n",
        "  \\[ Gini = 1 - \\sum_{i=1}^n p_i^2 \\]\n",
        "  - \\( p_i \\): Proportion of class \\( i \\) in the node.\n",
        "  - Measures the probability of misclassifying a randomly chosen instance if labeled randomly based on the node's class distribution.\n",
        "  \n",
        "- **Entropy**:\n",
        "  \\[ Entropy = - \\sum_{i=1}^n p_i \\log_2(p_i) \\]\n",
        "  - \\( p_i \\): Proportion of class \\( i \\) in the node.\n",
        "  - Measures the level of uncertainty or disorder in the node’s class distribution (based on information theory).\n",
        "\n",
        "### 2. **Range**\n",
        "- **Gini Impurity**: Ranges from 0 (pure, all one class) to 0.5 (maximum impurity for two equally distributed classes).\n",
        "- **Entropy**: Ranges from 0 (pure, all one class) to 1 (maximum impurity for two equally distributed classes).\n",
        "\n",
        "### 3. **Interpretation**\n",
        "- **Gini Impurity**: Represents the expected error rate if instances are classified randomly according to the node’s class distribution. It’s a measure of impurity based on squared probabilities.\n",
        "- **Entropy**: Quantifies the amount of uncertainty or information needed to describe the node’s class distribution. It’s rooted in information theory and reflects the average number of bits needed to encode class labels.\n",
        "\n",
        "### 4. **Computational Efficiency**\n",
        "- **Gini Impurity**: Faster to compute because it involves only squaring probabilities (\\( p_i^2 \\)) and simple arithmetic.\n",
        "- **Entropy**: Slower to compute due to the logarithm function (\\( \\log_2(p_i) \\)), which is more computationally intensive, especially for large datasets.\n",
        "\n",
        "### 5. **Sensitivity to Class Distribution**\n",
        "- **Gini Impurity**: Less sensitive to small changes in class probabilities, as it uses squared terms, which emphasize larger proportions.\n",
        "- **Entropy**: More sensitive to changes in class distribution due to the logarithmic function, which amplifies the impact of small probabilities.\n",
        "\n",
        "### 6. **Usage in Algorithms**\n",
        "- **Gini Impurity**: Commonly used in CART (Classification and Regression Trees) because of its simplicity and efficiency.\n",
        "- **Entropy**: Used in algorithms like ID3 and C4.5, which rely on Information Gain (or Gain Ratio) to select splits. Entropy is more aligned with information-theoretic approaches.\n",
        "\n",
        "### 7. **Practical Differences**\n",
        "- **Decision Boundary**: Both measures often lead to similar splits in practice, but Entropy may produce slightly more balanced trees due to its sensitivity to class distribution.\n",
        "- **Bias**: Gini tends to favor larger partitions, while Entropy (with Information Gain) may favor features with many values unless normalized (e.g., by Gain Ratio in C4.5).\n",
        "- **Performance**: The choice between Gini and Entropy rarely impacts the final model’s accuracy significantly, but Gini is preferred in implementations prioritizing speed (e.g., scikit-learn’s default for classification).\n",
        "\n",
        "### Example\n",
        "For a node with two classes (60% class A, 40% class B):\n",
        "- **Gini Impurity**:\n",
        "  \\[ Gini = 1 - (0.6^2 + 0.4^2) = 1 - (0.36 + 0.16) = 0.48 \\]\n",
        "- **Entropy**:\n",
        "  \\[ Entropy = - [0.6 \\log_2(0.6) + 0.4 \\log_2(0.4)] \\approx - [0.6 \\cdot (-0.737) + 0.4 \\cdot (-1.322)] \\approx 0.971 \\]\n",
        "\n",
        "### Summary\n",
        "- **Gini Impurity**: Simpler, faster, and slightly less sensitive to class distribution changes. Best for efficiency-driven implementations.\n",
        "- **Entropy**: More theoretically grounded, sensitive to class distribution, and computationally heavier. Preferred in algorithms emphasizing information gain.\n",
        "\n",
        "In most cases, the choice between Gini and Entropy depends on the algorithm and computational constraints, as both effectively guide decision tree splits toward purer nodes."
      ],
      "metadata": {
        "id": "pCL_G0GNuxNq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.7 What is the mathematical explanation behind Decision Trees?\n",
        "\n",
        "Ans - Decision trees are a machine learning model used for classification and regression tasks, built by recursively splitting the input space into regions based on feature values and making a decision based on the majority class or average value in that region. The mathematical explanation behind decision trees involves concepts like impurity measures, information gain, splitting criteria, and tree construction. Below, I provide a detailed mathematical framework for decision trees, focusing on classification (though similar principles apply to regression).\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Objective of Decision Trees**\n",
        "The goal is to partition the feature space into regions where instances belong to a single class (for classification) or have similar target values (for regression). Each node in the tree represents a decision rule based on a feature, and each leaf represents a prediction.\n",
        "\n",
        "Mathematically, given a dataset \\( D = \\{(x_i, y_i)\\}_{i=1}^N \\), where \\( x_i \\in \\mathbb{R}^d \\) is a feature vector with \\( d \\) features and \\( y_i \\) is the target (class label for classification), the decision tree seeks to minimize impurity or error in predictions by creating optimal splits.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Impurity Measures**\n",
        "Impurity measures quantify the heterogeneity of class labels in a node. The tree algorithm selects splits that reduce impurity. Common measures for classification are:\n",
        "\n",
        "- **Gini Impurity**:\n",
        "  \\[ Gini(D) = 1 - \\sum_{i=1}^C p_i^2 \\]\n",
        "  where \\( p_i \\) is the proportion of instances in node \\( D \\) belonging to class \\( i \\), and \\( C \\) is the number of classes. Gini measures the probability of misclassifying a randomly chosen instance if labeled randomly based on the node’s class distribution.\n",
        "\n",
        "- **Entropy**:\n",
        "  \\[ Entropy(D) = - \\sum_{i=1}^C p_i \\log_2(p_i) \\]\n",
        "  where \\( p_i \\log_2(p_i) = 0 \\) if \\( p_i = 0 \\). Entropy measures the uncertainty or information content of the node’s class distribution.\n",
        "\n",
        "- **Misclassification Error**:\n",
        "  \\[ Error(D) = 1 - \\max_i(p_i) \\]\n",
        "  This measures the fraction of instances that would be misclassified if all were assigned the majority class.\n",
        "\n",
        "For regression, impurity is typically measured using variance (mean squared error):\n",
        "\\[ Variance(D) = \\frac{1}{|D|} \\sum_{i \\in D} (y_i - \\bar{y})^2 \\]\n",
        "where \\( \\bar{y} \\) is the mean target value in the node.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Splitting Criteria and Information Gain**\n",
        "To decide which feature and threshold to split on, the algorithm evaluates all possible splits and selects the one that maximizes the reduction in impurity. The key metric for this is **Information Gain** (used with entropy) or the reduction in Gini impurity.\n",
        "\n",
        "For a node \\( D \\) split into \\( m \\) child nodes \\( \\{D_1, D_2, \\dots, D_m\\} \\) based on a feature \\( j \\) and threshold \\( t \\), the Information Gain is:\n",
        "\n",
        "\\[ IG(D, j, t) = Impurity(D) - \\sum_{k=1}^m \\frac{|D_k|}{|D|} Impurity(D_k) \\]\n",
        "\n",
        "- \\( Impurity(D) \\): Impurity of the parent node (e.g., Entropy or Gini).\n",
        "- \\( \\frac{|D_k|}{|D|} \\): Weight of child node \\( k \\), based on the number of instances in \\( D_k \\) relative to \\( D \\).\n",
        "- \\( Impurity(D_k) \\): Impurity of child node \\( k \\).\n",
        "\n",
        "For **Gini Impurity**, the reduction is:\n",
        "\\[ \\Delta Gini(D, j, t) = Gini(D) - \\sum_{k=1}^m \\frac{|D_k|}{|D|} Gini(D_k) \\]\n",
        "\n",
        "For **Entropy**, Information Gain is:\n",
        "\\[ IG(D, j, t) = Entropy(D) - \\sum_{k=1}^m \\frac{|D_k|}{|D|} Entropy(D_k) \\]\n",
        "\n",
        "For regression, the reduction in variance is used:\n",
        "\\[ \\Delta Variance(D, j, t) = Variance(D) - \\sum_{k=1}^m \\frac{|D_k|}{|D|} Variance(D_k) \\]\n",
        "\n",
        "The algorithm evaluates all possible features \\( j \\) and thresholds \\( t \\) (e.g., midpoints between consecutive feature values for continuous features) and selects the split \\((j, t)\\) that maximizes \\( IG \\) or \\( \\Delta Gini \\).\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Tree Construction**\n",
        "The decision tree is built recursively using the following steps:\n",
        "\n",
        "1. **Start at the Root**: The root node contains all instances \\( D \\).\n",
        "2. **Select Best Split**:\n",
        "   - For each feature \\( j \\) and possible threshold \\( t \\), compute the impurity reduction (e.g., Information Gain or Gini reduction).\n",
        "   - Choose the feature and threshold that maximize the reduction.\n",
        "3. **Split the Node**:\n",
        "   - Partition the data into child nodes based on the selected feature and threshold (e.g., \\( x_j \\leq t \\) for one child, \\( x_j > t \\) for the other).\n",
        "4. **Recurse**:\n",
        "   - Repeat the process for each child node until a stopping criterion is met:\n",
        "     - Maximum tree depth reached.\n",
        "     - Minimum number of instances in a node.\n",
        "     - Impurity below a threshold (node is sufficiently pure).\n",
        "     - No further improvement in impurity reduction.\n",
        "5. **Assign Leaf Nodes**:\n",
        "   - For classification, assign the majority class in the node as the prediction.\n",
        "   - For regression, assign the mean target value.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Mathematical Example**\n",
        "Suppose a node \\( D \\) has 100 instances: 60 of class A (\\( p_A = 0.6 \\)) and 40 of class B (\\( p_B = 0.4 \\)). Using Entropy:\n",
        "\\[ Entropy(D) = - [0.6 \\log_2(0.6) + 0.4 \\log_2(0.4)] \\approx 0.971 \\]\n",
        "\n",
        "Consider a split on feature \\( X \\) at threshold \\( t \\), creating two child nodes:\n",
        "- \\( D_1 \\): 60 instances (50 A, 10 B), so \\( p_A = 50/60 \\approx 0.833 \\), \\( p_B = 10/60 \\approx 0.167 \\).\n",
        "  \\[ Entropy(D_1) = - [0.833 \\log_2(0.833) + 0.167 \\log_2(0.167)] \\approx 0.65 \\]\n",
        "- \\( D_2 \\): 40 instances (0 A, 40 B), so \\( p_A = 0 \\), \\( p_B = 1 \\).\n",
        "  \\[ Entropy(D_2) = - [0 \\cdot \\log_2(0) + 1 \\cdot \\log_2(1)] = 0 \\]\n",
        "\n",
        "Weighted entropy after the split:\n",
        "\\[ \\text{Weighted Entropy} = \\frac{60}{100} \\cdot 0.65 + \\frac{40}{100} \\cdot 0 = 0.39 \\]\n",
        "\n",
        "Information Gain:\n",
        "\\[ IG = 0.971 - 0.39 = 0.581 \\]\n",
        "\n",
        "This gain is compared across all features and thresholds to select the best split.\n",
        "\n",
        "Using Gini Impurity for the same node:\n",
        "\\[ Gini(D) = 1 - (0.6^2 + 0.4^2) = 1 - (0.36 + 0.16) = 0.48 \\]\n",
        "- \\( D_1 \\): \\( Gini(D_1) = 1 - (0.833^2 + 0.167^2) \\approx 0.278 \\)\n",
        "- \\( D_2 \\): \\( Gini(D_2) = 1 - (0^2 + 1^2) = 0 \\)\n",
        "- Weighted Gini: \\( \\frac{60}{100} \\cdot 0.278 + \\frac{40}{100} \\cdot 0 \\approx 0.167 \\)\n",
        "- Gini reduction: \\( 0.48 - 0.167 = 0.313 \\)\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Additional Mathematical Considerations**\n",
        "- **Gain Ratio** (used in C4.5):\n",
        "  Information Gain can be biased toward features with many values. Gain Ratio normalizes it by the intrinsic information of the split:\n",
        "  \\[ \\text{Gain Ratio} = \\frac{IG(D, j, t)}{\\text{Split Information}} \\]\n",
        "  where \\( \\text{Split Information} = - \\sum_{k=1}^m \\frac{|D_k|}{|D|} \\log_2 \\left( \\frac{|D_k|}{|D|} \\right) \\).\n",
        "\n",
        "- **Overfitting**:\n",
        "  Without constraints, decision trees can perfectly fit the training data, leading to overfitting. Pruning techniques (e.g., cost-complexity pruning) minimize a cost function:\n",
        "  \\[ Cost = Error(T) + \\alpha |T| \\]\n",
        "  where \\( Error(T) \\) is the tree’s error, \\( |T| \\) is the number of leaves, and \\( \\alpha \\) is a regularization parameter.\n",
        "\n",
        "- **Continuous Features**:\n",
        "  For continuous features, thresholds are typically chosen as midpoints between sorted feature values, and the algorithm evaluates all possible thresholds to maximize impurity reduction.\n",
        "\n",
        "- **Regression Trees**:\n",
        "  Instead of impurity, regression trees minimize the weighted variance of child nodes. The split criterion is:\n",
        "  \\[ \\Delta Variance = Variance(D) - \\sum_{k=1}^m \\frac{|D_k|}{|D|} Variance(D_k) \\]\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **Prediction**\n",
        "For a new instance \\( x \\):\n",
        "- Traverse the tree from the root, following decision rules (e.g., \\( x_j \\leq t \\)) until reaching a leaf.\n",
        "- For classification, predict the majority class in the leaf.\n",
        "- For regression, predict the mean target value in the leaf.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. **Key Properties**\n",
        "- **Greedy Approach**: The algorithm makes locally optimal splits at each node, which may not guarantee a globally optimal tree.\n",
        "- **Non-linear Model**: Decision trees capture non-linear relationships by partitioning the feature space into rectangular regions.\n",
        "- **Interpretability**: The tree structure (decision rules) is easy to interpret, unlike many other models.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eIWtCigtwLDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.8 What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Ans - Pre-pruning in decision trees is a technique used to prevent overfitting by stopping the tree's growth early, before it becomes too complex, based on predefined constraints or criteria. It involves setting rules during the tree-building process to limit how the tree splits, ensuring a simpler model that generalizes better to unseen data. Below is a concise explanation of pre-pruning, including its mathematical and practical aspects.\n",
        "\n",
        "### How Pre-Pruning Works\n",
        "During tree construction, the algorithm recursively splits nodes to reduce impurity (e.g., Gini Impurity or Entropy). Pre-pruning halts this process at a node if certain conditions are met, preventing further splits and designating the node as a leaf. The leaf’s prediction is typically the majority class (for classification) or the mean target value (for regression).\n",
        "\n",
        "### Common Pre-Pruning Criteria\n",
        "Pre-pruning uses hyperparameters to control tree growth. These include:\n",
        "\n",
        "1. **Maximum Depth**:\n",
        "   - Limits the maximum number of levels in the tree.\n",
        "   - Mathematically, if the depth of a node (distance from the root) reaches a threshold \\( d_{\\text{max}} \\), no further splits are allowed.\n",
        "   - Effect: Prevents the tree from growing too deep, which reduces complexity and overfitting.\n",
        "\n",
        "2. **Minimum Samples per Split**:\n",
        "   - Requires a node to have at least \\( n_{\\text{min}} \\) instances to be eligible for splitting.\n",
        "   - If \\( |D| < n_{\\text{min}} \\), where \\( |D| \\) is the number of instances in the node, the node becomes a leaf.\n",
        "   - Effect: Avoids splits on small subsets of data, which are likely to capture noise.\n",
        "\n",
        "3. **Minimum Samples per Leaf**:\n",
        "   - Ensures each leaf node has at least \\( l_{\\text{min}} \\) instances.\n",
        "   - After a potential split, if any child node would have fewer than \\( l_{\\text{min}} \\) instances, the split is rejected.\n",
        "   - Effect: Ensures leaves have enough data to make reliable predictions.\n",
        "\n",
        "4. **Maximum Number of Leaf Nodes**:\n",
        "   - Caps the total number of leaves in the tree at \\( L_{\\text{max}} \\).\n",
        "   - If adding a split would exceed this limit, the node becomes a leaf.\n",
        "   - Effect: Directly controls the tree’s size.\n",
        "\n",
        "5. **Minimum Impurity Decrease**:\n",
        "   - Requires a split to reduce impurity (e.g., Gini or Entropy) by at least a threshold \\( \\gamma \\).\n",
        "   - For a split on feature \\( j \\) and threshold \\( t \\), the impurity reduction (e.g., Information Gain) must satisfy:\n",
        "     \\[ IG(D, j, t) = Impurity(D) - \\sum_{k=1}^m \\frac{|D_k|}{|D|} Impurity(D_k) \\geq \\gamma \\]\n",
        "     where \\( Impurity(D) \\) is the parent node’s impurity, and \\( \\frac{|D_k|}{|D|} Impurity(D_k) \\) is the weighted impurity of child nodes.\n",
        "   - Effect: Prevents splits that do not significantly improve node purity, avoiding trivial partitions.\n",
        "\n",
        "6. **Maximum Features**:\n",
        "   - Limits the number of features considered for splitting at each node (e.g., a random subset or \\( \\sqrt{d} \\), where \\( d \\) is the total number of features).\n",
        "   - Effect: Reduces complexity and introduces randomness, which can improve generalization (common in ensemble methods like Random Forests).\n",
        "\n",
        "### Mathematical Impact\n",
        "Pre-pruning simplifies the tree by reducing the number of nodes and leaves, which directly affects the model’s complexity. The tree’s hypothesis space is constrained, preventing it from fitting the training data too closely. For example:\n",
        "- Without pre-pruning, a tree might split until each leaf is pure (\\( Impurity(D) = 0 \\)), capturing noise in the data.\n",
        "- With pre-pruning, a node with \\( Entropy(D) = 0.8 \\) might be left as a leaf if \\( IG < \\gamma \\) or the node has fewer than \\( n_{\\text{min}} \\) instances, even if further splits could reduce impurity slightly.\n",
        "\n",
        "The trade-off is between bias and variance:\n",
        "- **High Bias**: A heavily pre-pruned tree (e.g., small \\( d_{\\text{max}} \\)) may underfit, failing to capture important patterns.\n",
        "- **Low Variance**: A simpler tree generalizes better, reducing sensitivity to noise in the training data.\n",
        "\n",
        "### Practical Example\n",
        "Suppose a node has 10 instances (6 class A, 4 class B), with \\( Entropy = 0.971 \\). A potential split on feature \\( X \\) yields:\n",
        "- Child 1: 5 instances (4 A, 1 B), \\( Entropy = 0.722 \\).\n",
        "- Child 2: 5 instances (2 A, 3 B), \\( Entropy = 0.971 \\).\n",
        "- Information Gain: \\( IG = 0.971 - \\left( \\frac{5}{10} \\cdot 0.722 + \\frac{5}{10} \\cdot 0.971 \\right) = 0.124 \\).\n",
        "\n",
        "If pre-pruning criteria include:\n",
        "- Minimum impurity decrease \\( \\gamma = 0.15 \\): The split is rejected (\\( IG = 0.124 < 0.15 \\)), and the node becomes a leaf.\n",
        "- Minimum samples per split \\( n_{\\text{min}} = 20 \\): The split is rejected (\\( |D| = 10 < 20 \\)), and the node becomes a leaf.\n",
        "\n",
        "The node predicts class A (majority class) without further splitting.\n",
        "\n",
        "### Advantages of Pre-Pruning\n",
        "- **Reduced Overfitting**: Limits tree complexity, improving generalization to test data.\n",
        "- **Faster Training**: Stops tree growth early, reducing computational cost.\n",
        "- **Simpler Model**: Produces interpretable trees with fewer nodes.\n",
        "\n",
        "### Disadvantages of Pre-Pruning\n",
        "- **Risk of Underfitting**: Strict criteria (e.g., low \\( d_{\\text{max}} \\)) may stop the tree from capturing important patterns.\n",
        "- **Threshold Sensitivity**: Choosing appropriate values for \\( d_{\\text{max}} \\), \\( n_{\\text{min}} \\), or \\( \\gamma \\) requires tuning, often via cross-validation.\n",
        "- **Greedy Limitation**: Pre-pruning may reject splits that seem suboptimal locally but could lead to better overall trees.\n",
        "\n",
        "### Comparison to Post-Pruning\n",
        "- **Pre-Pruning**: Stops tree growth during construction based on predefined rules.\n",
        "- **Post-Pruning** (or cost-complexity pruning): Grows a full tree, then removes branches that contribute little to accuracy, using a cost function like:\n",
        "  \\[ Cost = Error(T) + \\alpha |T| \\]\n",
        "  where \\( \\alpha \\) balances error and tree size \\( |T| \\).\n",
        "\n",
        "Pre-pruning is proactive and faster but may be overly restrictive, while post-pruning allows more exploration of splits but is computationally heavier.\n",
        "\n",
        "### Implementation\n",
        "In practice, pre-pruning is implemented via hyperparameters in libraries like scikit-learn:\n",
        "- `max_depth`: Limits tree depth.\n",
        "- `min_samples_split`: Minimum instances to split a node.\n",
        "- `min_samples_leaf`: Minimum instances per leaf.\n",
        "- `min_impurity_decrease`: Minimum impurity reduction for a split.\n",
        "\n"
      ],
      "metadata": {
        "id": "HYB_pQ8JwphI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.9 What is Post-Pruning in Decision Trees?\n",
        "\n",
        "Ans - Post-pruning, also known as cost-complexity pruning or backward pruning, is a technique used in decision trees to prevent overfitting by first growing a full, complex tree and then removing branches that contribute little to predictive accuracy. This results in a simpler tree that generalizes better to unseen data. Below is a concise mathematical and practical explanation of post-pruning in decision trees.\n",
        "\n",
        "### How Post-Pruning Works\n",
        "1. **Grow a Full Tree**: The decision tree is constructed without restrictions, splitting nodes recursively to minimize impurity (e.g., Gini Impurity or Entropy) until each leaf is pure or other minimal criteria (e.g., minimum samples per leaf) are met.\n",
        "2. **Prune Branches**: Subtrees are evaluated and removed if their removal does not significantly degrade performance, based on a cost function that balances accuracy and tree complexity.\n",
        "3. **Select Optimal Tree**: A sequence of pruned trees is generated, and the one with the best trade-off between error and simplicity is chosen, often using a validation set or cross-validation.\n",
        "\n",
        "### Mathematical Framework\n",
        "Post-pruning typically uses a **cost-complexity criterion** to decide which subtrees to prune. The cost of a tree \\( T \\) is defined as:\n",
        "\n",
        "\\[ Cost(T) = Error(T) + \\alpha |T| \\]\n",
        "\n",
        "where:\n",
        "- \\( Error(T) \\): The misclassification error (for classification) or mean squared error (for regression) of tree \\( T \\) on the training or validation data.\n",
        "  - For classification: \\( Error(T) = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}(y_i \\neq \\hat{y}_i) \\), where \\( \\mathbb{I} \\) is the indicator function, \\( y_i \\) is the true label, and \\( \\hat{y}_i \\) is the predicted label.\n",
        "  - For regression: \\( Error(T) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 \\).\n",
        "- \\( |T| \\): The number of leaf nodes in the tree (a measure of complexity).\n",
        "- \\( \\alpha \\): A regularization parameter (tuning parameter) that controls the trade-off between error and complexity. Larger \\( \\alpha \\) favors simpler trees.\n",
        "\n",
        "#### Pruning Process\n",
        "1. **Subtree Evaluation**: For each subtree rooted at an internal node, compute the cost of:\n",
        "   - Keeping the subtree (full tree cost).\n",
        "   - Replacing the subtree with a leaf (predicting the majority class or mean value of the subtree’s instances).\n",
        "2. **Cost Comparison**: Prune the subtree if its removal (replacing it with a leaf) results in a lower or equal cost:\n",
        "   \\[ Cost(\\text{leaf}) \\leq Cost(\\text{subtree}) \\]\n",
        "   where \\( Cost(\\text{leaf}) = Error(\\text{leaf}) + \\alpha \\cdot 1 \\) (since a leaf has one node).\n",
        "3. **Iterative Pruning**: Start with the full tree \\( T_0 \\). For each internal node, compute the effective \\( \\alpha \\) (called the cost-complexity parameter) where pruning the subtree equals keeping it:\n",
        "   \\[ \\alpha = \\frac{Error(\\text{leaf}) - Error(\\text{subtree})}{|T_{\\text{subtree}}| - 1} \\]\n",
        "   Prune the subtree with the smallest \\( \\alpha \\), creating a new tree \\( T_1 \\). Repeat until only the root remains, generating a sequence of trees \\( T_0, T_1, \\dots, T_k \\).\n",
        "4. **Select Best Tree**: Use a validation set or cross-validation to choose the tree with the lowest error on unseen data. The optimal \\( \\alpha \\) balances bias and variance.\n",
        "\n",
        "### Practical Example\n",
        "Suppose a tree \\( T \\) has a subtree with 3 leaves (\\( |T_{\\text{subtree}}| = 3 \\)) and a misclassification error of 0.05 on 100 instances (\\( Error(\\text{subtree}) = 0.05 \\)). Replacing the subtree with a leaf increases the error to 0.08 (\\( Error(\\text{leaf}) = 0.08 \\)). For \\( \\alpha = 0.01 \\):\n",
        "- Cost of subtree: \\( 0.05 + 0.01 \\cdot 3 = 0.08 \\).\n",
        "- Cost of leaf: \\( 0.08 + 0.01 \\cdot 1 = 0.09 \\).\n",
        "Since \\( 0.08 < 0.09 \\), keep the subtree. If \\( \\alpha = 0.02 \\):\n",
        "- Cost of subtree: \\( 0.05 + 0.02 \\cdot 3 = 0.11 \\).\n",
        "- Cost of leaf: \\( 0.08 + 0.02 \\cdot 1 = 0.10 \\).\n",
        "Since \\( 0.10 < 0.11 \\), prune the subtree.\n",
        "\n",
        "The effective \\( \\alpha \\) for pruning this subtree is:\n",
        "\\[ \\alpha = \\frac{0.08 - 0.05}{3 - 1} = 0.015 \\]\n",
        "If \\( \\alpha > 0.015 \\), the subtree is pruned.\n",
        "\n"
      ],
      "metadata": {
        "id": "LFdiw5esxGs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.10 What is the difference between Pre-Pruning and Post-Pruning?\n",
        "\n",
        "Ans - Pre-pruning and post-pruning are two techniques used in decision trees to prevent overfitting by controlling the tree's complexity. Pre-pruning stops the tree's growth early during construction, while post-pruning simplifies a fully grown tree by removing branches. Below is a concise comparison of their differences, focusing on their approach, mechanics, advantages, and disadvantages.\n",
        "\n",
        "### 1. **Definition and Approach**\n",
        "- **Pre-Pruning** (Early Stopping):\n",
        "  - Stops tree growth during construction by applying constraints before splitting a node.\n",
        "  - Uses predefined rules to decide whether a node should be split or turned into a leaf.\n",
        "  - Proactive approach: Prevents complexity from developing.\n",
        "\n",
        "- **Post-Pruning** (Cost-Complexity Pruning):\n",
        "  - Grows a full, unrestricted tree first, then removes branches that contribute little to predictive accuracy.\n",
        "  - Evaluates subtrees after construction and prunes them based on a cost-complexity trade-off.\n",
        "  - Reactive approach: Simplifies an already complex tree.\n",
        "\n",
        "### 2. **Mechanics**\n",
        "- **Pre-Pruning**:\n",
        "  - Applies criteria during tree construction, such as:\n",
        "    - Maximum tree depth (\\( d_{\\text{max}} \\)).\n",
        "    - Minimum samples per split (\\( n_{\\text{min}} \\)).\n",
        "    - Minimum samples per leaf (\\( l_{\\text{min}} \\)).\n",
        "    - Minimum impurity decrease (\\( \\gamma \\)), e.g., \\( IG(D, j, t) \\geq \\gamma \\), where \\( IG \\) is Information Gain or Gini reduction.\n",
        "  - If a criterion is not met (e.g., \\( |D| < n_{\\text{min}} \\) or \\( IG < \\gamma \\)), the node becomes a leaf, and no further splits occur.\n",
        "  - Example: If a node has 5 instances and \\( n_{\\text{min}} = 10 \\), it’s made a leaf without evaluating splits.\n",
        "\n",
        "- **Post-Pruning**:\n",
        "  - Grows a full tree until leaves are pure or minimal criteria are met.\n",
        "  - Evaluates each subtree using a cost-complexity criterion:\n",
        "    \\[ Cost(T) = Error(T) + \\alpha |T| \\]\n",
        "    where \\( Error(T) \\) is the misclassification error (classification) or mean squared error (regression), \\( |T| \\) is the number of leaves, and \\( \\alpha \\) is a regularization parameter.\n",
        "  - Prunes the subtree with the smallest effective \\( \\alpha \\), computed as:\n",
        "    \\[ \\alpha = \\frac{Error(\\text{leaf}) - Error(\\text{subtree})}{|T_{\\text{subtree}}| - 1} \\]\n",
        "  - Generates a sequence of pruned trees for increasing \\( \\alpha \\) and selects the best via validation data or cross-validation.\n",
        "\n",
        "### 3. **Timing**\n",
        "- **Pre-Pruning**: Occurs during tree construction, halting growth before the tree becomes too complex.\n",
        "- **Post-Pruning**: Occurs after the full tree is built, simplifying it by removing branches.\n",
        "\n",
        "### 4. **Computational Cost**\n",
        "- **Pre-Pruning**: Faster, as it stops tree growth early, reducing the number of splits evaluated.\n",
        "- **Post-Pruning**: More computationally intensive, as it requires building a full tree and evaluating multiple pruned versions.\n",
        "\n",
        "### 5. **Flexibility**\n",
        "- **Pre-Pruning**: Less flexible, as it relies on predefined thresholds that may prematurely stop useful splits. It’s a greedy approach that might miss globally optimal structures.\n",
        "- **Post-Pruning**: More flexible, as it explores all possible splits before pruning, allowing the algorithm to consider complex patterns that might be beneficial before simplifying.\n",
        "\n",
        "### 6. **Risk of Underfitting vs. Overfitting**\n",
        "- **Pre-Pruning**:\n",
        "  - Risk of **underfitting**: Strict criteria (e.g., low \\( d_{\\text{max}} \\) or high \\( \\gamma \\)) may stop the tree from capturing important patterns, leading to overly simple models.\n",
        "  - Less risk of overfitting, as the tree is constrained from the start.\n",
        "- **Post-Pruning**:\n",
        "  - Risk of **overfitting**: If \\( \\alpha \\) is too small, the pruned tree may still be too complex, retaining noise from the full tree.\n",
        "  - Better balance between underfitting and overfitting, as it uses validation data to select the optimal tree size.\n",
        "\n",
        "### 7. **Tuning Requirements**\n",
        "- **Pre-Pruning**: Requires tuning hyperparameters like \\( max_depth \\), \\( min_samples_split \\), \\( min_samples_leaf \\), or \\( min_impurity_decrease \\). These are often set heuristically or via cross-validation.\n",
        "- **Post-Pruning**: Requires tuning the cost-complexity parameter \\( \\alpha \\). The optimal \\( \\alpha \\) is typically chosen using a validation set or cross-validation to minimize error on unseen data.\n",
        "\n",
        "### 8. **Advantages**\n",
        "- **Pre-Pruning**:\n",
        "  - Faster training due to early stopping.\n",
        "  - Simpler implementation, as it avoids post-construction analysis.\n",
        "  - Reduces memory usage by limiting tree size during construction.\n",
        "- **Post-Pruning**:\n",
        "  - Potentially better performance, as it explores more splits before simplifying, capturing complex patterns that pre-pruning might miss.\n",
        "  - Data-driven pruning using validation data, leading to better generalization.\n",
        "  - Less sensitive to arbitrary threshold choices, as pruning decisions are based on actual performance.\n",
        "\n",
        "### 9. **Disadvantages**\n",
        "- **Pre-Pruning**:\n",
        "  - May underfit by stopping too early, missing important splits.\n",
        "  - Thresholds are hard to set optimally and may require extensive tuning.\n",
        "  - Greedy nature can lead to suboptimal trees.\n",
        "- **Post-Pruning**:\n",
        "  - Higher computational cost due to building and pruning a full tree.\n",
        "  - Requires validation data or cross-validation, increasing complexity.\n",
        "  - Risk of under-pruning if \\( \\alpha \\) is not tuned properly.\n",
        "\n",
        "### 10. **Practical Example**\n",
        "Suppose a node has 100 instances (60 class A, 40 class B, \\( Entropy = 0.971 \\)) and a potential split yields:\n",
        "- Child 1: 60 instances (50 A, 10 B, \\( Entropy = 0.65 \\)).\n",
        "- Child 2: 40 instances (0 A, 40 B, \\( Entropy = 0 \\)).\n",
        "- Information Gain: \\( IG = 0.971 - (0.6 \\cdot 0.65 + 0.4 \\cdot 0) = 0.581 \\).\n",
        "\n",
        "- **Pre-Pruning**: If \\( min_impurity_decrease = 0.6 \\), the split is rejected (\\( IG = 0.581 < 0.6 \\)), and the node becomes a leaf predicting class A.\n",
        "- **Post-Pruning**: The full tree is built, including this split. Later, the subtree is evaluated. If replacing it with a leaf increases error from 0.05 to 0.08 with 3 leaves, the effective \\( \\alpha = \\frac{0.08 - 0.05}{3 - 1} = 0.015 \\). If \\( \\alpha > 0.015 \\), the subtree is pruned.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "81ovCFX6zFio"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.11 What is a Decision Tree Regressor?\n",
        "\n",
        "Ans - A **Decision Tree Regressor** is a machine learning model used for regression tasks, where the goal is to predict a continuous target variable. It extends the decision tree framework, traditionally used for classification, to handle numerical outputs instead of discrete class labels. The model works by recursively partitioning the feature space into regions based on feature values and predicting the average target value for the instances in each region (leaf node). Below is a concise explanation of its mechanics, mathematical foundation, and key aspects.\n",
        "\n",
        "### How a Decision Tree Regressor Works\n",
        "1. **Tree Structure**:\n",
        "   - The tree consists of nodes and branches:\n",
        "     - **Root Node**: Contains the entire dataset.\n",
        "     - **Internal Nodes**: Represent decision rules based on a feature and threshold (e.g., \\( x_j \\leq t \\)).\n",
        "     - **Leaf Nodes**: Represent the final prediction, which is the mean (or sometimes median) of the target values for instances in that leaf.\n",
        "   - Each path from the root to a leaf defines a region in the feature space.\n",
        "\n",
        "2. **Splitting Process**:\n",
        "   - The algorithm selects a feature \\( j \\) and threshold \\( t \\) to split a node into child nodes, aiming to minimize the variance (or another error metric) of the target variable in the resulting child nodes.\n",
        "   - The process is repeated recursively until a stopping criterion is met (e.g., maximum depth, minimum samples per leaf).\n",
        "\n",
        "3. **Prediction**:\n",
        "   - For a new instance, the tree is traversed from the root to a leaf based on the feature values.\n",
        "   - The prediction is the mean target value of the training instances in that leaf.\n",
        "\n",
        "### Mathematical Foundation\n",
        "The decision tree regressor uses variance reduction as the primary criterion for selecting splits, unlike classification trees, which use impurity measures like Gini or Entropy.\n",
        "\n",
        "#### 1. **Variance as Impurity Measure**\n",
        "For a node \\( D \\) with \\( |D| \\) instances and target values \\( \\{y_i\\} \\), the variance is:\n",
        "\\[ Variance(D) = \\frac{1}{|D|} \\sum_{i \\in D} (y_i - \\bar{y})^2 \\]\n",
        "where \\( \\bar{y} = \\frac{1}{|D|} \\sum_{i \\in D} y_i \\) is the mean target value in the node.\n",
        "\n",
        "#### 2. **Splitting Criterion**\n",
        "For a split on feature \\( j \\) and threshold \\( t \\), the data is divided into child nodes \\( D_1 \\) (instances where \\( x_j \\leq t \\)) and \\( D_2 \\) (instances where \\( x_j > t \\)). The quality of the split is measured by the **variance reduction**:\n",
        "\\[ \\Delta Variance(D, j, t) = Variance(D) - \\sum_{k=1}^2 \\frac{|D_k|}{|D|} Variance(D_k) \\]\n",
        "where:\n",
        "- \\( Variance(D_k) = \\frac{1}{|D_k|} \\sum_{i \\in D_k} (y_i - \\bar{y}_k)^2 \\), with \\( \\bar{y}_k \\) as the mean of child node \\( k \\).\n",
        "- \\( \\frac{|D_k|}{|D|} \\): Weight of child node \\( k \\) based on the number of instances.\n",
        "\n",
        "The algorithm evaluates all possible features \\( j \\) and thresholds \\( t \\) (e.g., midpoints between sorted feature values) and selects the split that maximizes \\( \\Delta Variance \\).\n",
        "\n",
        "#### 3. **Leaf Prediction**\n",
        "For a leaf node \\( D_{\\text{leaf}} \\), the predicted value is:\n",
        "\\[ \\hat{y} = \\bar{y} = \\frac{1}{|D_{\\text{leaf}}|} \\sum_{i \\in D_{\\text{leaf}}} y_i \\]\n",
        "This minimizes the mean squared error within the leaf.\n",
        "\n",
        "#### 4. **Stopping Criteria**\n",
        "The tree stops growing when:\n",
        "- Maximum depth (\\( d_{\\text{max}} \\)) is reached.\n",
        "- Number of instances in a node is below a minimum (\\( n_{\\text{min}} \\)).\n",
        "- Number of instances in a leaf is below a minimum (\\( l_{\\text{min}} \\)).\n",
        "- Variance reduction is below a threshold.\n",
        "- Maximum number of leaves is reached.\n",
        "\n",
        "### Example\n",
        "Suppose a node has 4 instances with target values \\( [10, 20, 30, 40] \\), so \\( \\bar{y} = 25 \\), and:\n",
        "\\[ Variance(D) = \\frac{1}{4} [(10-25)^2 + (20-25)^2 + (30-25)^2 + (40-25)^2] = \\frac{1}{4} (225 + 25 + 25 + 225) = 125 \\]\n",
        "\n",
        "A split on feature \\( X \\) at threshold \\( t = 5 \\) divides the data into:\n",
        "- \\( D_1 \\): 2 instances (\\( [10, 20] \\)), mean = 15, \\( Variance(D_1) = \\frac{1}{2} [(10-15)^2 + (20-15)^2] = 25 \\).\n",
        "- \\( D_2 \\): 2 instances (\\( [30, 40] \\)), mean = 35, \\( Variance(D_2) = \\frac{1}{2} [(30-35)^2 + (40-35)^2] = 25 \\).\n",
        "- Weighted variance: \\( \\frac{2}{4} \\cdot 25 + \\frac{2}{4} \\cdot 25 = 25 \\).\n",
        "- Variance reduction: \\( \\Delta Variance = 125 - 25 = 100 \\).\n",
        "\n",
        "This split is compared to others, and the one with the highest variance reduction is chosen.\n",
        "\n",
        "### Pre-Pruning and Post-Pruning in Regression\n",
        "- **Pre-Pruning**: Stops tree growth early using constraints like maximum depth or minimum samples per split/leaf. For example, if a node has fewer than \\( n_{\\text{min}} \\) instances, it becomes a leaf.\n",
        "- **Post-Pruning**: Grows a full tree, then prunes subtrees using a cost-complexity criterion:\n",
        "  \\[ Cost(T) = MSE(T) + \\alpha |T| \\]\n",
        "  where \\( MSE(T) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 \\) is the mean squared error, and \\( |T| \\) is the number of leaves. Subtrees are pruned if replacing them with a leaf reduces the cost for a given \\( \\alpha \\).\n",
        "\n"
      ],
      "metadata": {
        "id": "o93qWjPNziZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.12 What are the advantages and disadvantages of Decision Trees?\n",
        "\n",
        "Ans - Decision trees are a popular machine learning model for both classification and regression tasks due to their interpretability and flexibility. Below is a concise summary of the **advantages** and **disadvantages** of decision trees, including considerations for both classification (e.g., using Gini Impurity or Entropy) and regression (e.g., using variance reduction).\n",
        "\n",
        "### Advantages of Decision Trees\n",
        "\n",
        "1. **Interpretability**:\n",
        "   - Decision trees are easy to understand and visualize. The tree structure (nodes, branches, and leaves) represents clear decision rules (e.g., \\( x_j \\leq t \\)), making it intuitive for non-technical users to interpret.\n",
        "   - Example: A tree can show how features like “age ≤ 30” lead to a specific prediction, aiding decision-making in domains like medicine or finance.\n",
        "\n",
        "2. **Handles Non-Linear Relationships**:\n",
        "   - Captures complex, non-linear patterns in data without requiring feature transformation or scaling.\n",
        "   - Unlike linear models, decision trees can model interactions between features naturally by creating hierarchical splits.\n",
        "\n",
        "3. **Works with Mixed Data Types**:\n",
        "   - Can handle both numerical and categorical features (with appropriate encoding for categorical data).\n",
        "   - Example: A tree can split on numerical thresholds (e.g., “income > 50K”) and categorical values (e.g., “color = blue”).\n",
        "\n",
        "4. **No Feature Scaling Required**:\n",
        "   - Decision trees are invariant to feature scaling or monotonic transformations, as splits depend on orderings (e.g., \\( x_j \\leq t \\)) rather than absolute values.\n",
        "   - This eliminates the need for normalization or standardization, unlike algorithms like SVM or neural networks.\n",
        "\n",
        "5. **Handles Missing Values** (in some implementations):\n",
        "   - Certain implementations (e.g., XGBoost, LightGBM) can handle missing values by treating them as a separate category or imputing them during splits.\n",
        "   - Example: Missing values can be sent to the child node that minimizes impurity.\n",
        "\n",
        "6. **Feature Importance**:\n",
        "   - Provides a measure of feature importance based on the total reduction in impurity (e.g., Gini, Entropy, or variance) across all splits involving a feature.\n",
        "   - Useful for feature selection and understanding which variables drive predictions.\n",
        "\n",
        "7. **Versatile for Classification and Regression**:\n",
        "   - Applicable to both classification (predicting discrete classes) and regression (predicting continuous values) by using appropriate criteria (e.g., Gini/Entropy for classification, variance for regression).\n",
        "\n",
        "8. **Fast Prediction**:\n",
        "   - Once trained, prediction is fast, requiring only a traversal from root to leaf (O(depth) complexity).\n",
        "\n",
        "### Disadvantages of Decision Trees\n",
        "\n",
        "1. **Overfitting**:\n",
        "   - Without constraints (e.g., pre-pruning or post-pruning), decision trees can grow overly complex, fitting noise in the training data and leading to poor generalization.\n",
        "   - Example: A deep tree with pure leaves may perfectly fit training data but fail on test data.\n",
        "   - Mitigation: Use pre-pruning (e.g., max_depth, min_samples_split) or post-pruning (cost-complexity pruning with \\( Cost = Error + \\alpha |T| \\)).\n",
        "\n",
        "2. **High Variance**:\n",
        "   - Small changes in the training data (e.g., adding/removing a few instances) can lead to significantly different tree structures, as the greedy splitting process is sensitive to data variations.\n",
        "   - Mitigation: Use ensemble methods like Random Forests or Gradient Boosting to stabilize predictions.\n",
        "\n",
        "3. **Greedy Algorithm Limitations**:\n",
        "   - The tree-building process is greedy, selecting the best split at each node based on local optimization (e.g., maximizing Information Gain or variance reduction). This may not yield the globally optimal tree.\n",
        "   - Example: A suboptimal early split can lead to a less effective overall structure.\n",
        "\n",
        "4. **Piecewise Constant Predictions (Regression)**:\n",
        "   - For regression, predictions are constant within each leaf (mean of the target values), which can lead to abrupt transitions and poor modeling of smooth functions.\n",
        "   - Example: A Decision Tree Regressor cannot capture a linear trend within a leaf, unlike linear regression.\n",
        "\n",
        "5. **Bias Toward Features with Many Values**:\n",
        "   - When using Information Gain (Entropy) or Gini Impurity, the algorithm may favor features with more unique values, as they can produce more splits and higher impurity reductions.\n",
        "   - Mitigation: Use Gain Ratio (in C4.5) or limit the number of features considered (e.g., max_features in Random Forests).\n",
        "\n",
        "6. **Poor Handling of Imbalanced Data**:\n",
        "   - Decision trees can be biased toward the majority class in imbalanced datasets, as impurity measures like Gini or Entropy prioritize larger classes.\n",
        "   - Mitigation: Use class weights, oversampling, or ensemble methods to balance the data.\n",
        "\n",
        "7. **Computational Cost for Large Datasets**:\n",
        "   - Training can be computationally expensive for large datasets with many features, as the algorithm evaluates all possible splits (features and thresholds) at each node.\n",
        "   - Complexity: O(N · d · log N) for numerical features, where N is the number of instances and d is the number of features, due to sorting for threshold selection.\n",
        "\n",
        "8. **Instability with Correlated Features**:\n",
        "   - Correlated features can lead to inconsistent splits, as the algorithm may arbitrarily choose one feature over another with similar predictive power.\n",
        "   - Mitigation: Feature selection or dimensionality reduction (e.g., PCA) before training.\n",
        "\n",
        "9. **Limited Extrapolation (Regression)**:\n",
        "   - Decision trees cannot extrapolate beyond the range of target values in the training data, as predictions are based on averages within leaves.\n",
        "   - Example: If the training data has target values between 0 and 100, the tree cannot predict values like 150.\n",
        "\n",
        "### Practical Considerations\n",
        "- **Pre-Pruning vs. Post-Pruning**:\n",
        "  - Pre-pruning (e.g., max_depth, min_samples_split) stops tree growth early, reducing overfitting but risking underfitting.\n",
        "  - Post-pruning (cost-complexity pruning with \\( \\alpha \\)) simplifies a full tree, balancing error and complexity but requiring more computation.\n",
        "- **Ensemble Methods**:\n",
        "  - Many disadvantages (e.g., overfitting, high variance) are mitigated by ensemble methods like Random Forests (bagging) or Gradient Boosting, which combine multiple trees for better performance.\n",
        "- **Implementation**:\n",
        "  - Libraries like scikit-learn provide `DecisionTreeClassifier` and `DecisionTreeRegressor` with parameters to control pruning (e.g., max_depth, ccp_alpha), making it easy to tune tree complexity.\n",
        "\n"
      ],
      "metadata": {
        "id": "Lu91rKLDz3hm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.13 How does a Decision Tree handle missing values?\n",
        "\n",
        "Ans - Decision trees can handle missing values in several ways, depending on the implementation and algorithm. Unlike some machine learning models that require explicit imputation of missing data, decision trees have built-in mechanisms to manage missing values during training and prediction. Below is a concise explanation of how decision trees handle missing values, including the mathematical and practical aspects.\n",
        "\n",
        "### 1. **Approaches to Handling Missing Values**\n",
        "Decision trees handle missing values using strategies that allow the algorithm to proceed with splitting or prediction without requiring complete data. Common approaches include:\n",
        "\n",
        "#### a. **Surrogate Splits**\n",
        "- **How It Works**: When a feature value is missing for an instance at a node, the tree uses alternative (surrogate) features that produce similar splits to the primary feature. These surrogates are chosen based on their ability to mimic the primary split’s outcome.\n",
        "- **Mechanics**:\n",
        "  - During training, for each split on feature \\( j \\) with threshold \\( t \\), the algorithm identifies surrogate features \\( k \\) that best replicate the split (e.g., by minimizing misclassification or variance).\n",
        "  - The quality of a surrogate split is evaluated by comparing its partitioning to the primary split’s partitioning, often using a metric like the proportion of correctly assigned instances.\n",
        "  - During prediction, if the primary feature \\( x_j \\) is missing, the tree checks surrogate features in order of priority until a non-missing value is found.\n",
        "- **Example**: If the primary split is “age ≤ 30” and age is missing, a surrogate split like “income ≤ 50K” might be used if it correlates strongly with the age split.\n",
        "- **Used In**: Algorithms like CART (Classification and Regression Trees).\n",
        "- **Advantage**: Robust, as it leverages correlated features without requiring imputation.\n",
        "- **Disadvantage**: Requires computing and storing surrogate splits, increasing complexity.\n",
        "\n",
        "#### b. **Missing Value as a Separate Category**\n",
        "- **How It Works**: Missing values are treated as a distinct category or value for the feature, allowing the tree to create a separate branch for missing data.\n",
        "- **Mechanics**:\n",
        "  - During training, for a feature \\( x_j \\), the algorithm includes “missing” as a valid value alongside other categories (for categorical features) or thresholds (for numerical features).\n",
        "  - The tree evaluates splits including the “missing” category, optimizing the impurity measure (e.g., Gini, Entropy, or variance).\n",
        "  - During prediction, instances with missing values follow the “missing” branch.\n",
        "- **Example**: For a categorical feature “color” with values [red, blue, missing], the tree might split into three branches: red, blue, and missing.\n",
        "- **Used In**: Some implementations of C4.5 and modern libraries like LightGBM.\n",
        "- **Advantage**: Simple and allows the tree to learn patterns specific to missing data.\n",
        "- **Disadvantage**: May lead to overfitting if missingness is not meaningful or if there are few missing values.\n",
        "\n",
        "#### c. **Fractional Instance Assignment (Probabilistic Splitting)**\n",
        "- **How It Works**: When a feature value is missing, the instance is fractionally assigned to all child nodes based on the proportion of non-missing instances going to each child.\n",
        "- **Mechanics**:\n",
        "  - During training, missing values are handled by sending the instance to all child nodes with weights proportional to the distribution of non-missing instances.\n",
        "  - For a node split into children \\( D_1, D_2, \\dots, D_m \\), the weight of an instance with missing feature \\( x_j \\) is distributed as \\( \\frac{|D_k|}{|D_{\\text{non-missing}}|} \\), where \\( |D_k| \\) is the number of non-missing instances in child \\( k \\).\n",
        "  - Impurity (e.g., Gini, Entropy, or variance) is computed using these weighted instances.\n",
        "  - During prediction, the instance follows all branches, and the final prediction is a weighted average (for regression) or majority vote (for classification) across leaves.\n",
        "- **Example**: If 60% of non-missing instances go to the left child and 40% to the right, a missing-value instance is assigned 0.6 weight to the left and 0.4 to the right.\n",
        "- **Used In**: Algorithms like C4.5 and some implementations of decision trees.\n",
        "- **Advantage**: Handles missing values without requiring imputation or additional splits.\n",
        "- **Disadvantage**: Increases computational complexity and may dilute the impact of individual instances.\n",
        "\n",
        "#### d. **Imputation Before Training**\n",
        "- **How It Works**: Missing values are imputed (e.g., with the mean, median, or mode) before training the tree, treating the imputed dataset as complete.\n",
        "- **Mechanics**:\n",
        "  - Numerical features: Replace missing values with the mean or median of the feature.\n",
        "  - Categorical features: Replace missing values with the mode or a placeholder category.\n",
        "  - The tree is then trained on the imputed dataset as usual.\n",
        "- **Example**: If “age” has missing values, replace them with the mean age (e.g., 35) before building the tree.\n",
        "- **Used In**: Common in basic implementations or when preprocessing is preferred.\n",
        "- **Advantage**: Simplifies tree construction by eliminating missing values.\n",
        "- **Disadvantage**: Imputation may introduce bias or obscure patterns related to missingness.\n",
        "\n",
        "#### e. **Ignoring Missing Values**\n",
        "- **How It Works**: Instances with missing values for a feature are excluded when evaluating splits for that feature, but included for other features.\n",
        "- **Mechanics**:\n",
        "  - During training, the impurity reduction (e.g., Information Gain or variance reduction) is calculated using only non-missing instances for the feature being considered.\n",
        "  - During prediction, if a feature is missing, the algorithm may use a default branch (e.g., the majority branch) or rely on other features.\n",
        "- **Used In**: Simple implementations or when missingness is rare.\n",
        "- **Advantage**: Straightforward and avoids assumptions about missing data.\n",
        "- **Disadvantage**: Reduces the effective sample size for splits, potentially leading to less robust trees.\n",
        "\n",
        "### 2. **Mathematical Considerations**\n",
        "The handling of missing values affects the impurity calculation and split selection. For example:\n",
        "- **Gini Impurity** (classification):\n",
        "  \\[ Gini(D) = 1 - \\sum_{i=1}^C p_i^2 \\]\n",
        "  If using fractional assignment, \\( p_i \\) is computed with weighted instances, where weights for missing-value instances are distributed across children.\n",
        "- **Entropy** (classification):\n",
        "  \\[ Entropy(D) = - \\sum_{i=1}^C p_i \\log_2(p_i) \\]\n",
        "  Similarly, weights adjust the class proportions \\( p_i \\).\n",
        "- **Variance** (regression):\n",
        "  \\[ Variance(D) = \\frac{1}{|D|} \\sum_{i \\in D} (y_i - \\bar{y})^2 \\]\n",
        "  Missing values are handled by weighting instances or excluding them when computing \\( \\bar{y} \\) and variance.\n",
        "\n",
        "The goal is to maximize impurity reduction (e.g., \\( \\Delta Variance = Variance(D) - \\sum_{k} \\frac{|D_k|}{|D|} Variance(D_k) \\)) while accounting for missing values in a consistent way.\n",
        "\n",
        "### 3. **Practical Implementation**\n",
        "Modern libraries and algorithms implement missing value handling differently:\n",
        "- **Scikit-learn** (`DecisionTreeClassifier`, `DecisionTreeRegressor`):\n",
        "  - Does not natively handle missing values; requires preprocessing (e.g., imputation) before training.\n",
        "  - Users must impute missing values (e.g., using `SimpleImputer`) or remove instances/features with missing data.\n",
        "- **XGBoost/LightGBM**:\n",
        "  - Treat missing values as a separate category by default, allowing the algorithm to learn optimal splits for missing data.\n",
        "  - During prediction, missing values follow the branch learned during training (e.g., the one that minimizes impurity).\n",
        "- **C4.5**:\n",
        "  - Uses fractional instance assignment, distributing instances with missing values across children based on the proportion of non-missing instances.\n",
        "- **CART**:\n",
        "  - Often uses surrogate splits to handle missing values, leveraging correlated features.\n",
        "\n",
        "### 4. **Advantages of Missing Value Handling in Decision Trees**\n",
        "- **Flexibility**: Methods like surrogate splits or treating missing values as a category allow trees to learn patterns without requiring imputation.\n",
        "- **Preserves Information**: Unlike imputation, which may introduce bias, methods like fractional assignment or separate categories retain information about missingness.\n",
        "- **Robustness**: Surrogate splits leverage correlated features, making the model robust to missing data in predictive features.\n",
        "\n",
        "### 5. **Disadvantages**\n",
        "- **Complexity**: Surrogate splits and fractional assignment increase computational and memory requirements.\n",
        "- **Bias Risk**: Treating missing values as a separate category may overfit if missingness is not meaningful or occurs rarely.\n",
        "- **Implementation Dependency**: Not all libraries support advanced missing value handling (e.g., scikit-learn requires imputation), limiting portability.\n",
        "\n",
        "### 6. **Example**\n",
        "Suppose a node has 100 instances, 20 of “\n",
        "\n",
        "System: It looks like your message was cut off. You mentioned an example with a node having 100 instances, 20 of which have some characteristic. Would you like me to continue the example or clarify any specific aspect of how decision trees handle missing values? For now, I’ll provide a brief example to illustrate one approach (e.g., treating missing values as a separate category) and ensure the response is complete.\n",
        "\n",
        "---\n",
        "\n",
        "### Example: Handling Missing Values with Separate Category\n",
        "Suppose a node has 100 instances for a classification task with two classes (A: 60, B: 40), and we’re evaluating a split on feature \\( X \\), which is numerical. Among the 100 instances, 20 have missing values for \\( X \\). The non-missing values are split at threshold \\( t = 5 \\):\n",
        "\n",
        "- **Left Child** (\\( X \\leq 5 \\)): 50 instances (40 A, 10 B).\n",
        "- **Right Child** (\\( X > 5 \\)): 30 instances (10 A, 20 B).\n",
        "- **Missing Values**: 20 instances (10 A, 10 B).\n",
        "\n",
        "**Treating Missing as a Separate Category**:\n",
        "- Create three child nodes: \\( X \\leq 5 \\), \\( X > 5 \\), and “missing.”\n",
        "- Calculate Gini Impurity for the parent node:\n",
        "  \\[ Gini(D) = 1 - (0.6^2 + 0.4^2) = 1 - (0.36 + 0.16) = 0.48 \\]\n",
        "- Calculate Gini for children:\n",
        "  - Left: \\( Gini(D_1) = 1 - \\left( \\left(\\frac{40}{50}\\right)^2 + \\left(\\frac{10}{50}\\right)^2 \\right) = 1 - (0.64 + 0.04) = 0.32 \\).\n",
        "  - Right: \\( Gini(D_2) = 1 - \\left( \\left(\\frac{10}{30}\\right)^2 + \\left(\\frac{20}{30}\\right)^2 \\right) = 1 - (0.111 + 0.444) \\approx 0.445 \\).\n",
        "  - Missing: \\( Gini(D_{\\text{missing}}) = 1 - \\left( \\left(\\frac{10}{20}\\right)^2 + \\left(\\frac{10}{20}\\right)^2 \\right) = 1 - (0.25 + 0.25) = 0.5 \\).\n",
        "- Weighted Gini after split:\n",
        "  \\[ \\text{Weighted Gini} = \\frac{50}{100} \\cdot 0.32 + \\frac{30}{100} \\cdot 0.445 + \\frac{20}{100} \\cdot 0.5 = 0.16 + 0.1335 + 0.1 = 0.3935 \\]\n",
        "- Gini reduction: \\( 0.48 - 0.3935 = 0.0865 \\).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g_iaBMfm0gYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.14 How does a Decision Tree handle categorical features?\n",
        "\n",
        "Ans - Decision trees can effectively handle categorical features by incorporating them into the splitting process, allowing the model to create decision rules based on categorical values. Unlike numerical features, which are typically split using thresholds (e.g., \\( x_j \\leq t \\)), categorical features require specific handling to account for their discrete, non-ordinal nature. Below is a concise explanation of how decision trees handle categorical features, including the mathematical and practical aspects.\n",
        "\n",
        "### 1. **Approaches to Handling Categorical Features**\n",
        "The handling of categorical features depends on the decision tree algorithm and implementation. Common methods include:\n",
        "\n",
        "#### a. **Binary Splits (One-vs-Rest)**\n",
        "- **How It Works**: For a categorical feature with \\( K \\) unique values, the tree evaluates splits by grouping one category (or a subset of categories) against the rest.\n",
        "- **Mechanics**:\n",
        "  - For each category \\( k \\), the algorithm considers a binary split: “feature = \\( k \\)” vs. “feature ≠ \\( k \\).”\n",
        "  - The split is evaluated using an impurity measure (e.g., Gini Impurity, Entropy for classification, or variance for regression).\n",
        "  - The best category or subset of categories is chosen to maximize impurity reduction.\n",
        "- **Example**: For a feature “color” with values {red, blue, green}, possible splits include:\n",
        "  - “color = red” vs. “color ∈ {blue, green}”\n",
        "  - “color = blue” vs. “color ∈ {red, green}”\n",
        "  - “color = green” vs. “color ∈ {red, blue}”\n",
        "- **Used In**: Algorithms like CART (Classification and Regression Trees).\n",
        "- **Advantage**: Simple and computationally efficient for features with few categories.\n",
        "- **Disadvantage**: May be suboptimal for features with many categories, as it tests only binary partitions.\n",
        "\n",
        "#### b. **Multi-Way Splits**\n",
        "- **How It Works**: The tree creates one branch for each unique value of the categorical feature, resulting in a multi-way split.\n",
        "- **Mechanics**:\n",
        "  - For a categorical feature with \\( K \\) values, the node splits into \\( K \\) child nodes, one for each category.\n",
        "  - The impurity reduction (e.g., Information Gain or variance reduction) is computed for the split:\n",
        "    \\[ IG(D, j) = Impurity(D) - \\sum_{k=1}^K \\frac{|D_k|}{|D|} Impurity(D_k) \\]\n",
        "    where \\( D_k \\) is the subset of instances with category \\( k \\), and \\( Impurity \\) is Gini, Entropy, or variance.\n",
        "  - The split is compared to other features’ splits to select the best.\n",
        "- **Example**: For “color” with {red, blue, green}, the node splits into three branches: one for red, one for blue, and one for green.\n",
        "- **Used In**: Algorithms like C4.5 and ID3.\n",
        "- **Advantage**: Naturally handles all categories, capturing their distinct effects.\n",
        "- **Disadvantage**: Can lead to overfitting or fragmented data when \\( K \\) is large, as each child node may have few instances.\n",
        "\n",
        "#### c. **Subset Splits (Grouping Categories)**\n",
        "- **How It Works**: The algorithm groups categories into subsets to form binary or multi-way splits, optimizing for impurity reduction.\n",
        "- **Mechanics**:\n",
        "  - For a feature with \\( K \\) categories, the algorithm evaluates combinations of categories to form subsets (e.g., {red, blue} vs. {green}).\n",
        "  - The best subset split is chosen to maximize impurity reduction, often using a greedy search or heuristic to reduce the number of combinations (since \\( 2^K - 2 \\) possible binary splits exist).\n",
        "  - This is computationally intensive for high-cardinality features, so approximations are often used.\n",
        "- **Example**: For “color” with {red, blue, green, yellow}, a split might be “color ∈ {red, blue}” vs. “color ∈ {green, yellow}.”\n",
        "- **Used In**: Advanced implementations or when explicitly enabled (e.g., in some libraries).\n",
        "- **Advantage**: More flexible than one-vs-rest, potentially finding better splits.\n",
        "- **Disadvantage**: Computationally expensive for features with many categories.\n",
        "\n",
        "#### d. **Encoding Categorical Features (Preprocessing)**\n",
        "- **How It Works**: Categorical features are converted to numerical values before training, typically using one-hot encoding or ordinal encoding.\n",
        "- **Mechanics**:\n",
        "  - **One-Hot Encoding**: Each category becomes a binary feature (e.g., “color = red” becomes a column with 1 for red, 0 otherwise). The tree treats these as numerical features, splitting on 0 or 1.\n",
        "  - **Ordinal Encoding**: Categories are mapped to integers (e.g., red = 0, blue = 1, green = 2), and the tree splits on thresholds (e.g., color ≤ 1).\n",
        "  - The tree then applies standard numerical splitting (e.g., \\( x_j \\leq t \\)).\n",
        "- **Example**: For “color” with {red, blue, green}, one-hot encoding creates three features: “is_red,” “is_blue,” “is_green.” A split might be “is_red ≤ 0.5” (equivalent to “color ≠ red”).\n",
        "- **Used In**: Libraries like scikit-learn, which often require numerical input.\n",
        "- **Advantage**: Simplifies tree construction by converting categorical features to numerical ones.\n",
        "- **Disadvantage**: One-hot encoding increases dimensionality (problematic for high-cardinality features), and ordinal encoding assumes an order that may not exist.\n",
        "\n",
        "### 2. **Mathematical Considerations**\n",
        "The goal of splitting on a categorical feature is to maximize impurity reduction, similar to numerical features. For a node \\( D \\), the impurity reduction for a categorical feature \\( j \\) is:\n",
        "\\[ \\Delta Impurity(D, j) = Impurity(D) - \\sum_{k} \\frac{|D_k|}{|D|} Impurity(D_k) \\]\n",
        "where:\n",
        "- \\( Impurity(D) \\): Gini, Entropy (classification), or variance (regression) of the parent node.\n",
        "- \\( D_k \\): Subset of instances for category \\( k \\) (or a subset of categories).\n",
        "- Common measures:\n",
        "  - **Gini Impurity**: \\( Gini(D) = 1 - \\sum_{i=1}^C p_i^2 \\), where \\( p_i \\) is the proportion of class \\( i \\).\n",
        "  - **Entropy**: \\( Entropy(D) = - \\sum_{i=1}^C p_i \\log_2(p_i) \\).\n",
        "  - **Variance** (regression): \\( Variance(D) = \\frac{1}{|D|} \\sum_{i \\in D} (y_i - \\bar{y})^2 \\).\n",
        "\n",
        "For multi-way splits, all \\( K \\) categories are considered. For binary or subset splits, the algorithm evaluates combinations to find the partition that maximizes \\( \\Delta Impurity \\).\n",
        "\n",
        "### 3. **Handling High-Cardinality Categorical Features**\n",
        "High-cardinality features (e.g., thousands of categories, like ZIP codes) pose challenges:\n",
        "- **Multi-Way Splits**: Create many child nodes, leading to data fragmentation and overfitting.\n",
        "- **Binary/Subset Splits**: Evaluating all possible subsets is computationally expensive (\\( O(2^K) \\)).\n",
        "- **Mitigation**:\n",
        "  - Use greedy algorithms to test only promising subsets (e.g., based on impurity reduction).\n",
        "  - Apply one-hot encoding and limit the number of features considered (e.g., `max_features` in scikit-learn).\n",
        "  - Group rare categories into an “other” category to reduce cardinality.\n",
        "\n",
        "### 4. **Practical Implementation**\n",
        "- **Scikit-learn** (`DecisionTreeClassifier`, `DecisionTreeRegressor`):\n",
        "  - Requires categorical features to be numerically encoded (e.g., one-hot encoding or ordinal encoding) before training.\n",
        "  - One-hot encoding is common, but it increases dimensionality, so sparse matrices or feature selection may be needed.\n",
        "- **XGBoost/LightGBM/CatBoost**:\n",
        "  - Natively handle categorical features by treating them as discrete values, often using multi-way or subset splits.\n",
        "  - CatBoost, for example, optimizes categorical feature handling by ordering categories based on target statistics (e.g., mean target value for regression) to improve splits.\n",
        "- **C4.5/ID3**:\n",
        "  - Support multi-way splits for categorical features, creating one branch per category.\n",
        "\n",
        "### 5. **Example**\n",
        "Suppose a node has 100 instances (60 class A, 40 class B) and a categorical feature “color” with values {red, blue, green}:\n",
        "- **Parent Gini**: \\( Gini(D) = 1 - (0.6^2 + 0.4^2) = 0.48 \\).\n",
        "- **Multi-Way Split**:\n",
        "  - Red: 40 instances (30 A, 10 B), \\( Gini = 1 - (0.75^2 + 0.25^2) = 0.375 \\).\n",
        "  - Blue: 30 instances (20 A, 10 B), \\( Gini = 1 - (0.667^2 + 0.333^2) \\approx 0.444 \\).\n",
        "  - Green: 30 instances (10 A, 20 B), \\( Gini = 1 - (0.333^2 + 0.667^2) \\approx 0.444 \\).\n",
        "  - Weighted Gini: \\( \\frac{40}{100} \\cdot 0.375 + \\frac{30}{100} \\cdot 0.444 + \\frac{30}{100} \\cdot 0.444 = 0.416 \\).\n",
        "  - Gini reduction: \\( 0.48 - 0.416 = 0.064 \\).\n",
        "- **Binary Split** (e.g., “color = red” vs. “color ≠ red”):\n",
        "  - Red: Same as above.\n",
        "  - Not Red: 60 instances (30 A, 30 B), \\( Gini = 1 - (0.5^2 + 0.5^2) = 0.5 \\).\n",
        "  - Weighted Gini: \\( \\frac{40}{100} \\cdot 0.375 + \\frac{60}{100} \\cdot 0.5 = 0.45 \\).\n",
        "  - Gini reduction: \\( 0.48 - 0.45 = 0.03 \\).\n",
        "- The multi-way split is preferred if it yields higher impurity reduction.\n",
        "\n",
        "### 6. **Advantages of Handling Categorical Features**\n",
        "- **Natural Fit**: Decision trees can directly use categorical features without assuming an order, unlike some models (e.g., linear regression).\n",
        "- **Flexibility**: Multi-way or subset splits capture complex patterns in categorical data.\n",
        "- **No Scaling Needed**: Categorical splits are invariant to scaling, simplifying preprocessing.\n",
        "\n",
        "### 7. **Disadvantages**\n",
        "- **High Cardinality**: Features with many categories (e.g., >100) can lead to overfitting or computational inefficiency due to excessive splits or dimensionality (if one-hot encoded).\n",
        "- **Bias in Impurity Measures**: Information Gain or Gini may favor high-cardinality features, as they offer more splitting options. Mitigation: Use Gain Ratio (C4.5) or limit features.\n",
        "- **Preprocessing Overhead**: Some implementations (e.g., scikit-learn) require encoding, increasing memory usage for one-hot encoding.\n",
        "\n"
      ],
      "metadata": {
        "id": "DgPGgOQ90_f4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.15 What are some real-world applications of Decision Trees?\n",
        "\n",
        "Ans - Decision trees are widely used in various real-world applications due to their interpretability, ability to handle both numerical and categorical data, and effectiveness in modeling non-linear relationships. Below is a concise overview of some key real-world applications of decision trees, highlighting their practical use across industries.\n",
        "\n",
        "### 1. **Medical Diagnosis**\n",
        "- **Application**: Decision trees are used to diagnose diseases based on patient symptoms, medical history, and test results.\n",
        "- **Example**: Predicting whether a patient has a specific condition (e.g., diabetes) using features like blood sugar levels, BMI, age, and family history. The tree’s rules (e.g., “if blood sugar > 126 mg/dL and BMI > 30, then diabetes”) are interpretable by doctors.\n",
        "- **Why Decision Trees?**: Their transparency helps medical professionals understand and trust the decision-making process, and they handle mixed data types (e.g., categorical symptoms, numerical lab results).\n",
        "\n",
        "### 2. **Credit Risk Assessment**\n",
        "- **Application**: Banks and financial institutions use decision trees to evaluate the creditworthiness of loan applicants.\n",
        "- **Example**: Predicting whether an applicant is likely to default based on features like credit score, income, debt-to-income ratio, and employment status. A tree might split on “credit score ≤ 600” to flag high-risk applicants.\n",
        "- **Why Decision Trees?**: They provide clear decision rules for approving or rejecting loans and can handle missing values or categorical data (e.g., employment type).\n",
        "\n",
        "### 3. **Customer Churn Prediction**\n",
        "- **Application**: Businesses use decision trees to predict which customers are likely to stop using their services or products.\n",
        "- **Example**: A telecom company might use features like contract length, monthly charges, customer complaints, and usage patterns to predict churn. The tree could identify high-risk customers (e.g., “if contract < 6 months and complaints > 2, then likely to churn”).\n",
        "- **Why Decision Trees?**: They identify key factors driving churn, enabling targeted retention strategies, and their feature importance helps prioritize interventions.\n",
        "\n",
        "### 4. **Fraud Detection**\n",
        "- **Application**: Decision trees are used to detect fraudulent transactions in banking, insurance, or e-commerce.\n",
        "- **Example**: Identifying suspicious credit card transactions based on features like transaction amount, location, time, and user behavior. A tree might flag transactions with “amount > $1,000 and location ≠ usual” as potential fraud.\n",
        "- **Why Decision Trees?**: They handle imbalanced datasets (fraud is rare) and provide interpretable rules for fraud detection systems.\n",
        "\n",
        "### 5. **Marketing and Customer Segmentation**\n",
        "- **Application**: Decision trees help segment customers for targeted marketing campaigns or personalize recommendations.\n",
        "- **Example**: An e-commerce platform might use purchase history, browsing behavior, and demographics to segment customers into groups (e.g., “high-value” vs. “occasional buyers”). A tree could split on “average purchase > $100 and visits/week > 3.”\n",
        "- **Why Decision Trees?**: Their ability to handle categorical features (e.g., product categories) and produce clear segments makes them ideal for marketing strategies.\n",
        "\n",
        "### 6. **Manufacturing and Quality Control**\n",
        "- **Application**: Decision trees are used to predict equipment failures or detect defects in manufacturing processes.\n",
        "- **Example**: In a factory, a decision tree regressor might predict machine downtime based on sensor data (e.g., temperature, vibration, usage hours). A classification tree could identify defective products based on production parameters.\n",
        "- **Why Decision Trees?**: They handle numerical sensor data and provide actionable rules for maintenance or quality checks.\n",
        "\n",
        "### 7. **Recommender Systems**\n",
        "- **Application**: Decision trees support recommendation engines by predicting user preferences or item ratings.\n",
        "- **Example**: A streaming service might use user demographics, viewing history, and genre preferences to recommend movies. A tree could predict high ratings for “if age < 30 and genre = action.”\n",
        "- **Why Decision Trees?**: They handle categorical data (e.g., genres) and can be combined with ensemble methods for better performance.\n",
        "\n",
        "### 8. **Environmental Modeling**\n",
        "- **Application**: Decision trees are used in environmental science to predict outcomes like air quality, species classification, or climate impacts.\n",
        "- **Example**: Predicting whether a species is endangered based on habitat features, population size, and human activity. A tree might use rules like “if habitat loss > 50% and population < 1,000, then endangered.”\n",
        "- **Why Decision Trees?**: They model complex, non-linear relationships in environmental data and are interpretable for policymakers.\n",
        "\n",
        "### 9. **Image and Text Classification**\n",
        "- **Application**: Decision trees classify images or text in tasks like spam detection or medical image analysis.\n",
        "- **Example**: In spam email detection, a tree might use features like word frequency, sender reputation, and email length to classify emails as “spam” or “not spam.”\n",
        "- **Why Decision Trees?**: They work with preprocessed features (e.g., word counts) and are often used as part of ensemble methods like Random Forests for better accuracy.\n",
        "\n",
        "### 10. **Sports Analytics**\n",
        "- **Application**: Decision trees predict game outcomes, player performance, or strategy effectiveness.\n",
        "- **Example**: Predicting whether a basketball team will win based on player stats, team rankings, and game conditions. A tree might split on “average points scored > 100 and opponent rank < 5.”\n",
        "- **Why Decision Trees?**: They handle mixed data (e.g., numerical stats, categorical team names) and provide insights for coaches or analysts.\n",
        "\n",
        "### Mathematical Context\n",
        "Decision trees in these applications use:\n",
        "- **Classification**: Splits based on impurity measures like Gini Impurity (\\( Gini = 1 - \\sum p_i^2 \\)) or Entropy (\\( Entropy = - \\sum p_i \\log_2(p_i) \\)).\n",
        "- **Regression**: Splits based on variance reduction (\\( \\Delta Variance = Variance(D) - \\sum \\frac{|D_k|}{|D|} Variance(D_k) \\)).\n",
        "- **Pruning**: Pre-pruning (e.g., max_depth) or post-pruning (\\( Cost = Error + \\alpha |T| \\)) to prevent overfitting, ensuring robust models for real-world data.\n",
        "\n",
        "### Advantages in Real-World Applications\n",
        "- **Interpretability**: Clear rules (e.g., “if income > $50K, then approve loan”) are valuable in regulated industries like finance and healthcare.\n",
        "- **Flexibility**: Handle mixed data types, missing values, and non-linear relationships.\n",
        "- **Feature Importance**: Identify key drivers (e.g., “blood sugar” in diabetes prediction), aiding decision-making.\n",
        "\n",
        "### Challenges in Real-World Applications\n",
        "- **Overfitting**: Deep trees may fit noise, requiring pruning or ensemble methods (e.g., Random Forests, XGBoost).\n",
        "- **High Variance**: Sensitive to data changes, mitigated by ensembles.\n",
        "- **High-Cardinality Features**: Categorical features with many values (e.g., ZIP codes) can lead to complex trees, requiring preprocessing or subset splits.\n",
        "\n"
      ],
      "metadata": {
        "id": "BTNLtWi-2m5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q.16 Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy.\n",
        "\n",
        "Ans -\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and testing sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Decision Tree Classifier Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "sUwsF6Lj3XFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.17 Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances.\n",
        "\n",
        "Ans -\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train a Decision Tree Classifier using Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "importances = clf.feature_importances_\n",
        "for feature, importance in zip(feature_names, importances):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "_Ng_ixvl3X5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.18 Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the model accuracy.\n",
        "\n",
        "Ans -\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and test sets (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree Classifier using 'entropy'\n",
        "clf = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy using Entropy: {accuracy:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "eIU7US3C3v6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.19 Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error (MSE).\n",
        "\n",
        "Ans -\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split into training and test sets (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(criterion='squared_error', random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Evaluate using Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error on test data: {mse:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "HX6q1qMh4AES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.20 Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz?\n",
        "\n",
        "Ans -\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.model_selection import train_test_split\n",
        "import graphviz\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "feature_names = iris.feature_names\n",
        "class_names = iris.target_names\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train the classifier\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Export the tree in DOT format\n",
        "dot_data = export_graphviz(\n",
        "    clf,\n",
        "    out_file=None,\n",
        "    feature_names=feature_names,\n",
        "    class_names=class_names,\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    special_characters=True\n",
        ")\n",
        "\n",
        "# Render and display the tree using graphviz\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"iris_tree\", format=\"png\", cleanup=True)\n",
        "graph.view()\n",
        "\n"
      ],
      "metadata": {
        "id": "1YSzWT4l4LK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.21 Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its accuracy with a fully grown tree.\n",
        "\n",
        "Ans -\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Classifier with max_depth=3\n",
        "clf_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Fully grown tree (no depth limit)\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print comparison\n",
        "print(f\"Accuracy with max_depth=3       : {accuracy_limited:.2f}\")\n",
        "print(f\"Accuracy with full depth (default): {accuracy_full:.2f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fctUFXR54T_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.22 Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its accuracy with a default tree.\n",
        "\n",
        "Ans -\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Classifier with min_samples_split=5\n",
        "clf_custom = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
        "clf_custom.fit(X_train, y_train)\n",
        "y_pred_custom = clf_custom.predict(X_test)\n",
        "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
        "\n",
        "# Default classifier (min_samples_split=2)\n",
        "clf_default = DecisionTreeClassifier(random_state=42)\n",
        "clf_default.fit(X_train, y_train)\n",
        "y_pred_default = clf_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Print the comparison\n",
        "print(f\"Accuracy with min_samples_split=5: {accuracy_custom:.2f}\")\n",
        "print(f\"Accuracy with default settings    : {accuracy_default:.2f}\")"
      ],
      "metadata": {
        "id": "C1ZESZLH4j6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.23 Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its accuracy with unscaled data.ArithmeticError\n",
        "\n",
        "Ans -\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# --- Without Feature Scaling ---\n",
        "clf_unscaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = clf_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# --- With Feature Scaling ---\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "clf_scaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print comparison\n",
        "print(f\"Accuracy without scaling: {accuracy_unscaled:.2f}\")\n",
        "print(f\"Accuracy with scaling   : {accuracy_scaled:.2f}\")"
      ],
      "metadata": {
        "id": "BwL8jFjo4jw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.24 Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification.\n",
        "\n",
        "Ans -\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Wrap the Decision Tree in One-vs-Rest strategy\n",
        "ovr_classifier = OneVsRestClassifier(DecisionTreeClassifier(random_state=42))\n",
        "ovr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = ovr_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"OvR Decision Tree Classifier Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "bZHriSiY418J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.25 Write a Python program to train a Decision Tree Classifier and display the feature importance scores.\n",
        "\n",
        "Ans -\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Display feature importance scores\n",
        "importances = clf.feature_importances_\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(feature_names, importances):\n",
        "    print(f\"{feature:30s}: {importance:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "FW9KaZdk4-r2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.26 Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance with an unrestricted tree.\n",
        "\n",
        "Ans -\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Regressor with max_depth=5\n",
        "reg_limited = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "reg_limited.fit(X_train, y_train)\n",
        "y_pred_limited = reg_limited.predict(X_test)\n",
        "mse_limited = mean_squared_error(y_test, y_pred_limited)\n",
        "\n",
        "# Unrestricted regressor\n",
        "reg_full = DecisionTreeRegressor(random_state=42)\n",
        "reg_full.fit(X_train, y_train)\n",
        "y_pred_full = reg_full.predict(X_test)\n",
        "mse_full = mean_squared_error(y_test, y_pred_full)\n",
        "\n",
        "# Output results\n",
        "print(f\"MSE with max_depth=5       : {mse_limited:.2f}\")\n",
        "print(f\"MSE with no depth limit    : {mse_full:.2f}\")"
      ],
      "metadata": {
        "id": "HNFjFXGL5S5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.27 Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy.\n",
        "\n",
        "Ans -\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train initial tree to get effective alphas\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas\n",
        "\n",
        "# Train trees for each alpha value\n",
        "clfs = []\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n",
        "    clf.fit(X_train, y_train)\n",
        "    clfs.append(clf)\n",
        "\n",
        "# Remove the trivial tree with only root\n",
        "clfs = clfs[:-1]\n",
        "ccp_alphas = ccp_alphas[:-1]\n",
        "\n",
        "# Evaluate accuracy\n",
        "train_acc = [accuracy_score(y_train, clf.predict(X_train)) for clf in clfs]\n",
        "test_acc = [accuracy_score(y_test, clf.predict(X_test)) for clf in clfs]\n",
        "\n",
        "# Plotting accuracy vs alpha\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(ccp_alphas, train_acc, marker='o', label=\"Train Accuracy\", drawstyle=\"steps-post\")\n",
        "plt.plot(ccp_alphas, test_acc, marker='o', label=\"Test Accuracy\", drawstyle=\"steps-post\")\n",
        "plt.xlabel(\"ccp_alpha (pruning strength)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Effect of Cost Complexity Pruning on Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z-TQdaYW5coD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.28 Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "\n",
        "Ans -\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate using Precision, Recall, and F1-Score\n",
        "report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(report)"
      ],
      "metadata": {
        "id": "W_QyKDhe5mxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.29 Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn.\n",
        "\n",
        "Ans -\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "class_names = iris.target_names\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot using seaborn\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Decision Tree Confusion Matrix (Iris Dataset)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5ObajLKR5wXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.30 Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values for max_depth and min_samples_split.\n",
        "\n",
        "Ans -\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Base model\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Grid search with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=dt,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and estimator\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
      ],
      "metadata": {
        "id": "NJrlb4sU6HY4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}